{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa02786a-8dc1-4128-a384-1572ba06fe83",
   "metadata": {},
   "source": [
    "# Ground Truth\n",
    "\n",
    "A project to build foundational market data sets from camera streams utilizing image recognition AI.\n",
    "\n",
    "![Demo Screenshot](./GTD_SN0.png)\n",
    "\n",
    "\n",
    "Project Features:\n",
    "* Live camera stream\n",
    "  * Selection of portion of camera image to use in AI processing\n",
    "* Object Recogntion AI\n",
    "* Graph of Object Recognitions over Time\n",
    "\n",
    "With a provided video stream, let's track and analyze the appearance of object classes over time.\n",
    "\n",
    "Utilizing [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics), we can classify images for objects the model has been trained on. This can be used to build a dataset of observed objects over time. This can then be plotted to generate observed objects over time graphs.\n",
    "\n",
    "## Camera Considerations\n",
    "* Camera Perspective/Placement: cameras are best placed anti-parallel to the movement of objects to be observed; this reduces object blur\n",
    "* Camera Resolution and Observation Area: a higher resolution camera and/or more of the camera's frame being occupied by the tracked objects will more reliable classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5246c7-4b88-498a-8b17-6c974ce21445",
   "metadata": {},
   "source": [
    "This notebook needs a camera stream that will return an JPG image frame with a simple GET request. For a Raspberry Pi, [ayufan's camerastreamer](https://github.com/ayufan/camera-streamer) is a fantastic choice. The URL for that camera stream should be set in the `CAMERA_STREAM` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bbe29-3985-4c96-8925-1c47798ae5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERA_STREAM = \"http://localhost:8080/snapshot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff2538-f6be-4782-96a5-cdecd7c45cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from time import sleep\n",
    "\n",
    "import flask\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import requests\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f658a-33a9-4f5d-9509-49e919c3266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece306c4-c694-4b2e-afca-e58aac1ef761",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"GroundTruth.json\", \"r\") as f:\n",
    "        groundTruthConfig = json.loads(f.read())\n",
    "        groundTruthConfig['DATA'] = [(datetime.fromisoformat(d[0]), d[1]) for d in groundTruthConfig['DATA']]\n",
    "except FileNotFoundError:\n",
    "    groundTruthConfig = {\n",
    "        \"CLASSES_OF_INTEREST\": [2, 5, 6, 7],\n",
    "        \"COUNT_CLASSES\": [2, 7],\n",
    "        \"ACTIVE_ZONE\": [(0, 0), (1920, 0), (1920, 1080), (0, 1080)],\n",
    "        \"DATA\": [(datetime.utcnow(), {c: 0 for c in [2, 5, 6, 7]})]\n",
    "    }\n",
    "\n",
    "CLASSES_OF_INTEREST = groundTruthConfig['CLASSES_OF_INTEREST']\n",
    "COUNT_CLASSES = groundTruthConfig['COUNT_CLASSES']\n",
    "ACTIVE_ZONE = groundTruthConfig['ACTIVE_ZONE']\n",
    "DATA = groundTruthConfig['DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6abe5-cb80-469f-a132-9e166b6cb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropToActiveZone(image):\n",
    "    pts = np.int32(ACTIVE_ZONE)\n",
    "    mask = np.zeros(image.shape[:2], np.uint8)\n",
    "    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
    "    dst = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return dst\n",
    "        \n",
    "def drawActiveZone(image):\n",
    "    pts = np.int32(ACTIVE_ZONE)\n",
    "    azOverlaidImage = image.copy()\n",
    "    return cv2.polylines(azOverlaidImage, [pts], isClosed=True, color=(100, 255, 0), thickness=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca9939-b8be-4acb-a86f-1e68a7dee1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images(count=5, rotate=None):\n",
    "    images = []\n",
    "    try:\n",
    "        for i in range(count):\n",
    "            resp = requests.get(CAMERA_STREAM, stream=True).raw\n",
    "            images.append(resp)\n",
    "        images = [np.asarray(bytearray(resp.read()), dtype=\"uint8\") for resp in images]\n",
    "        images = [cv2.imdecode(image, cv2.IMREAD_COLOR) for image in images]\n",
    "        images = [cv2.resize(camImage, (1920, 1080)) for camImage in images]\n",
    "        if rotate:\n",
    "            images = [cv2.rotate(camImage, rotate) for camImage in images]\n",
    "\n",
    "        return images\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to collect image: {e}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874807a8-6987-4a02-a736-1601e7d4da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demostrateCollectionAndRecognition():\n",
    "    images = collect_images()\n",
    "    aResults = []\n",
    "    for image in images:\n",
    "        results = model.track(image, persist=True, verbose=False)[0]\n",
    "        aResults.append(results)\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    for ax, i in zip(axes, aResults):\n",
    "        ax.imshow(i.plot())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e1d3a-a88b-4acd-981f-dffaca80bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATED_IMAGE = None\n",
    "\n",
    "def captureAndAnalyzeImage():\n",
    "    while True:\n",
    "        camImage = collect_images(1)[-1]\n",
    "        results = model(cropToActiveZone(camImage), verbose=False)[0]\n",
    "    \n",
    "        annotatedImage = camImage.copy()\n",
    "        recognitions = 0\n",
    "        if len(results.boxes) > 0:\n",
    "            print(f\"Detections at {datetime.utcnow()} - {[results.names[int(i)] for i in results.boxes.cls]}\")\n",
    "        recognitions = {c: 0 for c in CLASSES_OF_INTEREST}\n",
    "        for box in results.boxes:\n",
    "            if box.cls[0] in CLASSES_OF_INTEREST:\n",
    "                label = results.names[int(box.cls[0])]\n",
    "                print(f\"Saw {label}: {box.xyxy}\")\n",
    "                x1, y1, x2, y2 = [int(i) for i in box.xyxy[0]]\n",
    "                color = (0, 255, 0) if box.cls[0] in COUNT_CLASSES else (255, 0, 0)\n",
    "                cv2.rectangle(annotatedImage, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.rectangle(annotatedImage, (x1, y1 - 25), (x2, y1), color, -1)\n",
    "                cv2.putText(annotatedImage, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                recognitions[int(box.cls[0])] += 1\n",
    "        DATA.append((datetime.utcnow(), recognitions))\n",
    "\n",
    "        global ANNOTATED_IMAGE\n",
    "        ret, encodedImage = cv2.imencode('.jpg', drawActiveZone(annotatedImage))\n",
    "        ANNOTATED_IMAGE = encodedImage.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b40b37-6bf9-4125-a2ee-60cc1cc2b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camStream():\n",
    "    while ANNOTATED_IMAGE is None:\n",
    "            sleep(5)\n",
    "\n",
    "    while True:\n",
    "        yield(b'--frame\\r\\n'\n",
    "              b'Content-Type: image/jpg\\r\\n\\r\\n' + ANNOTATED_IMAGE + b'\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab22d1-90bc-4589-b271-12c6b7553b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYZER_IMAGE = None\n",
    "\n",
    "def buildAnalyzer():\n",
    "    while True:\n",
    "        x, y = zip(*DATA)\n",
    "        y = [sum([v for k, v in yi.items() if k in COUNT_CLASSES]) for yi in y]\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(x, y)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Number of Recognized Trucks\")\n",
    "        fig.autofmt_xdate()\n",
    "        img_bytes = BytesIO()\n",
    "        plt.savefig(img_bytes, format=\"jpg\")\n",
    "        img_bytes.seek(0)\n",
    "        plt.close(fig)\n",
    "\n",
    "        global ANALYZER_IMAGE\n",
    "        ANALYZER_IMAGE = img_bytes.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf1501-81c1-4524-bbaf-0d7a46c6d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataStream():\n",
    "    while ANALYZER_IMAGE is None:\n",
    "        sleep(5)\n",
    "\n",
    "    while True:\n",
    "        yield(b'--frame\\r\\n'\n",
    "              b'Content-Type: image/jpg\\r\\n\\r\\n' + ANALYZER_IMAGE + b'\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055e2ba-cfbb-4da0-9541-418a97086654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_oot(*args, **kwargs):\n",
    "    with open('templates/OOT.html', 'r') as f:\n",
    "        ootTemplate = f.read().replace(\"{active_zone}\", json.dumps(ACTIVE_ZONE))\n",
    "    return ootTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade232f5-55b9-4605-afbf-078afc75b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = flask.Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return flask.Response(generate_oot(), mimetype=\"text/html\")\n",
    "\n",
    "@app.route('/observer')\n",
    "def observer():\n",
    "    return flask.Response(camStream(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/analyzer')\n",
    "def analyzer():\n",
    "    return flask.Response(dataStream(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/active_zone', methods=['POST'])\n",
    "def set_active_zone():\n",
    "    new_active_zone = json.loads(flask.request.form['az'])\n",
    "    if new_active_zone == []:\n",
    "        new_active_zone = [(0, 0), (1920, 0), (1920, 1080), (0, 1080)]\n",
    "    global ACTIVE_ZONE\n",
    "    ACTIVE_ZONE = new_active_zone\n",
    "    return json.dumps(ACTIVE_ZONE)\n",
    "\n",
    "def saveState(snapshot=False):\n",
    "    global DATA\n",
    "    state = json.dumps({\n",
    "        \"CLASSES_OF_INTEREST\": CLASSES_OF_INTEREST,\n",
    "        \"COUNT_CLASSES\": COUNT_CLASSES,\n",
    "        \"ACTIVE_ZONE\": ACTIVE_ZONE,\n",
    "        \"DATA\": [(d[0].isoformat(), d[1]) for d in DATA]\n",
    "    })\n",
    "    with open(\"GroundTruth.json\", \"w\") as f:\n",
    "        f.write(state)\n",
    "\n",
    "    if snapshot:\n",
    "        with open(f\"GroundTruth-{datetime.utcnow().isoformat()}.json\", \"w\") as f:\n",
    "            f.write(state)\n",
    "        DATA =  [(datetime.utcnow(), {c: 0 for c in CLASSES_OF_INTEREST})]\n",
    "\n",
    "@app.route('/save', methods=['POST'])\n",
    "def saveRequest():\n",
    "    saveState(snapshot=True)\n",
    "    return \"saved\"\n",
    "\n",
    "@app.route('/clear_data', methods=['POST'])\n",
    "def clearData():\n",
    "    global DATA\n",
    "    DATA =  [(datetime.utcnow(), {c: 0 for c in CLASSES_OF_INTEREST})]\n",
    "    saveState()\n",
    "    return \"cleared\"\n",
    "\n",
    "@app.route('/bootstrap.min.css', methods=['GET'])\n",
    "def getBSCSS():\n",
    "    with open(\"templates/bootstrap.min.css\", \"r\") as f:\n",
    "        bscss = f.read()\n",
    "    return flask.Response(bscss, mimetype=\"text/css\")\n",
    "    \n",
    "@app.route('/bootstrap.min.js', methods=['GET'])\n",
    "def getBSJS():\n",
    "    with open(\"templates/bootstrap.min.js\", \"r\") as f:\n",
    "        bsjs = f.read()\n",
    "    return flask.Response(bsjs, mimetype=\"application/javascript\")\n",
    "    \n",
    "@app.route('/htmx.min.js', methods=['GET'])\n",
    "def getHTMX():\n",
    "    with open(\"templates/htmx.min.js\", \"r\") as f:\n",
    "        htmx = f.read()\n",
    "    return flask.Response(htmx, mimetype=\"application/javascript\")\n",
    "\n",
    "\n",
    "## Periodic tasks for capturing images and analyzing data\n",
    "scheduler = BackgroundScheduler(daemon=True)\n",
    "scheduler.add_job(captureAndAnalyzeImage, 'interval', seconds=2)\n",
    "scheduler.add_job(buildAnalyzer, 'interval', seconds=5)\n",
    "scheduler.add_job(saveState, 'interval', seconds=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122217ba-2495-4c4b-8675-96673dd257cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Launching Observer Server on 7777\")\n",
    "    scheduler.start()\n",
    "    app.run(host=\"0.0.0.0\", port=7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0255f-da4b-4637-ac0c-c44f669c4e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
